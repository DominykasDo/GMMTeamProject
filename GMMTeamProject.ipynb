{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJe4ZSIGTASB"
      },
      "source": [
        "# Download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "lk_CGWATSEaL"
      },
      "outputs": [],
      "source": [
        "# Download imports\n",
        "!pip install objaverse --upgrade -q\n",
        "\n",
        "import objaverse\n",
        "import objaverse.xl as oxl\n",
        "\n",
        "# Downloading annotations\n",
        "annotations = oxl.get_annotations(\n",
        "  download_dir=\"objaverse\"\n",
        ")\n",
        "\n",
        "# Downloading objects\n",
        "num_samples = 100\n",
        "sketchfab_annotations = annotations[annotations[\"source\"] == \"sketchfab\"]\n",
        "sketchfab_samples = sketchfab_annotations.sample(n=num_samples)\n",
        "\n",
        "oxl.download_objects(\n",
        "  objects = sketchfab_samples,\n",
        "  download_dir = \"objaverse\",\n",
        "  processes = 2\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6M82HYFTCvA"
      },
      "source": [
        "# Conversion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0jCPsTSKTHYi"
      },
      "outputs": [],
      "source": [
        "# Conversion imports\n",
        "!pip -q install trimesh[easy] pyrender\n",
        "\n",
        "import os\n",
        "os.environ[\"PYOPENGL_PLATFORM\"] = \"egl\"\n",
        "\n",
        "import trimesh\n",
        "import pyrender\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import traceback\n",
        "\n",
        "# Conversion settings\n",
        "input_folder = \"/content/objaverse/hf-objaverse-v1/glbs\"\n",
        "images_folder = \"/content/pngs\"\n",
        "images_folder_rotated = \"/content/pngs_rotated\"\n",
        "\n",
        "image_width = 256\n",
        "image_height = 256\n",
        "STATIC_DEGREES = 45\n",
        "max_glb_size_bytes = 100 * 1024 * 1024  # 100 MB\n",
        "\n",
        "os.makedirs(images_folder, exist_ok=True)\n",
        "os.makedirs(images_folder_rotated, exist_ok=True)\n",
        "\n",
        "static_radians = np.radians(STATIC_DEGREES)\n",
        "vertical_axis = [1, 0, 0]\n",
        "rotation_matrix = trimesh.transformations.rotation_matrix(static_radians, vertical_axis)\n",
        "\n",
        "def convert_glb_to_images(glb_path, output_path_original, output_path_rotated):\n",
        "  try:\n",
        "    # Loading .glb in scene\n",
        "    scene = trimesh.load(glb_path, force=\"scene\", process=False)\n",
        "    if isinstance(scene, trimesh.Trimesh):\n",
        "      scene = trimesh.Scene(scene)\n",
        "    if not isinstance(scene, trimesh.Scene):\n",
        "      return False\n",
        "\n",
        "    # Fixing 2-channel (LA) textures if object has them\n",
        "    for geometry in scene.geometry.values():\n",
        "      visuals = getattr(geometry, \"visual\", None)\n",
        "      if isinstance(visuals, trimesh.visual.texture.TextureVisuals):\n",
        "        material = getattr(visuals, \"material\", None)\n",
        "        texture = getattr(material, \"baseColorTexture\", None) if hasattr(material, \"baseColorTexture\") else getattr(material, \"image\", None)\n",
        "        if isinstance(texture, Image.Image) and len(texture.getbands()) == 2:\n",
        "          converted_texture = texture.convert(\"RGBA\")\n",
        "          if hasattr(material, \"baseColorTexture\"):\n",
        "            material.baseColorTexture = converted_texture\n",
        "          elif hasattr(material, \"image\"):\n",
        "            material.image = converted_texture\n",
        "\n",
        "    # Centering\n",
        "    if scene.geometry:\n",
        "      try:\n",
        "        scene.apply_translation(-scene.bounds.mean(axis=0))\n",
        "      except:\n",
        "        pass\n",
        "\n",
        "    # Applying random rotation\n",
        "    random_angle = np.random.rand() * 2 * np.pi\n",
        "    random_axis = trimesh.unitize(np.random.rand(3) - 0.5)\n",
        "    random_rotation_matrix = trimesh.transformations.rotation_matrix(random_angle, random_axis)\n",
        "    scene.apply_transform(random_rotation_matrix)\n",
        "\n",
        "    # Trying to get or set a camera pose\n",
        "    try:\n",
        "      camera_pose = scene.camera_transform\n",
        "    except:\n",
        "      scene.set_camera(angles=np.random.rand(3) * (np.pi / 4) - (np.pi / 8), fov=60)\n",
        "      camera_pose = scene.camera_transform\n",
        "\n",
        "    # Setting up offscreen renderer\n",
        "    renderer = pyrender.OffscreenRenderer(viewport_width=image_width, viewport_height=image_height)\n",
        "\n",
        "    # Rendering and saving an image from the scene\n",
        "    def render_and_save(trimesh_scene, output_path):\n",
        "      pyrender_scene = pyrender.Scene.from_trimesh_scene(\n",
        "        trimesh_scene, ambient_light=[0.15, 0.15, 0.15], bg_color=[0, 0, 0, 0]\n",
        "      )\n",
        "      if not pyrender_scene.main_camera_node:\n",
        "        pyrender_scene.add(\n",
        "          pyrender.PerspectiveCamera(yfov=np.pi / 3.0, aspectRatio=image_width / image_height),\n",
        "          pose=camera_pose\n",
        "        )\n",
        "      if not pyrender_scene.lights:\n",
        "        pyrender_scene.add(\n",
        "          pyrender.DirectionalLight(color=[1, 1, 1], intensity=300),\n",
        "          pose=camera_pose\n",
        "        )\n",
        "      color, _ = renderer.render(pyrender_scene, flags=pyrender.RenderFlags.RGBA)\n",
        "      Image.fromarray(color, \"RGBA\").save(output_path)\n",
        "\n",
        "    # First image: random rotation\n",
        "    render_and_save(scene, output_path_original)\n",
        "\n",
        "    # Second image: with extra defined rotation\n",
        "    scene.apply_transform(rotation_matrix)\n",
        "    render_and_save(scene, output_path_rotated)\n",
        "\n",
        "    renderer.delete()\n",
        "    return True\n",
        "\n",
        "  except Exception:\n",
        "    traceback.print_exc()\n",
        "    return False\n",
        "\n",
        "# Finding all .glb files\n",
        "glb_files = [\n",
        "  os.path.join(root, filename)\n",
        "  for root, _, filenames in os.walk(input_folder)\n",
        "  for filename in filenames if filename.lower().endswith(\".glb\")\n",
        "]\n",
        "\n",
        "print(f\"Found {len(glb_files)} .glb files.\")\n",
        "\n",
        "success_count = 0\n",
        "failure_count = 0\n",
        "\n",
        "for glb_path in tqdm(glb_files, desc=\"Processing .glb files\"):\n",
        "  base_filename = os.path.splitext(os.path.basename(glb_path))[0]\n",
        "  output_path_original = os.path.join(images_folder, base_filename + \".png\")\n",
        "  output_path_rotated = os.path.join(images_folder_rotated, base_filename + \".png\")\n",
        "\n",
        "  # Skipping large files\n",
        "  if os.path.getsize(glb_path) > max_glb_size_bytes:\n",
        "    failure_count += 1\n",
        "    continue\n",
        "\n",
        "  success = convert_glb_to_images(glb_path, output_path_original, output_path_rotated)\n",
        "  success_count += success\n",
        "  failure_count += not success\n",
        "\n",
        "# Summary\n",
        "print(f\"\\nDone: {success_count} succeeded, {failure_count} failed.\")\n",
        "print(f\"Outputs saved to: '{images_folder}' and '{images_folder_rotated}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-1qptgTb8nG"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9tRq0kL-cAB2",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Evaluation imports\n",
        "import os\n",
        "import sys\n",
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "from huggingface_hub import snapshot_download\n",
        "from skimage.metrics import (\n",
        "  peak_signal_noise_ratio as psnr_metric,\n",
        "  structural_similarity as ssim_metric,\n",
        "  mean_squared_error as mse_metric,\n",
        "  normalized_root_mse as nrmse_metric\n",
        ")\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "import PIL\n",
        "from PIL import Image\n",
        "\n",
        "try:\n",
        "  !wget -nc https://github.com/DominykasDo/GMMTeamProject/raw/refs/heads/main/pipeline_zero1to3.py\n",
        "  from pipeline_zero1to3 import Zero1to3StableDiffusionPipeline\n",
        "except ImportError:\n",
        "  raise Exception(f\"'pipeline_zero1to3.py' not found.\")\n",
        "\n",
        "try:\n",
        "  !pip install lpips\n",
        "  import lpips\n",
        "  lpips_alex = lpips.LPIPS(net=\"alex\")\n",
        "  LPIPS_AVAILABLE = True\n",
        "except ImportError:\n",
        "  LPIPS_AVAILABLE = False\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "# Placeholder for models\n",
        "MODELS = {}\n",
        "\n",
        "# Redefinitions (Since Colab could crash after lpips import)\n",
        "STATIC_DEGREES = 45\n",
        "images_folder = \"/content/pngs\"\n",
        "images_folder_rotated = \"/content/pngs_rotated\"\n",
        "\n",
        "# Data loading utilities\n",
        "def load_image_rgba(image_path):\n",
        "  if not os.path.exists(image_path):\n",
        "    raise Exception(f\"Image {image_path} not found.\")\n",
        "  return np.array(Image.open(image_path).convert(\"RGBA\"))\n",
        "\n",
        "def get_image_files(directory):\n",
        "  if not os.path.isdir(directory):\n",
        "    raise Exception(f\"Directory {directory} not found.\")\n",
        "  return sorted([file_name for file_name in os.listdir(directory) if file_name.lower().endswith(\".png\")])\n",
        "\n",
        "def convert_to_rgb_with_whitened_transparency(input_image_rgba):\n",
        "  if not isinstance(input_image_rgba, PIL.PngImagePlugin.PngImageFile):\n",
        "    input_image = Image.fromarray(input_image_rgba, mode=\"RGBA\")\n",
        "  else:\n",
        "    input_image = input_image_rgba\n",
        "  white_bg = Image.new(\"RGB\", input_image.size, (255, 255, 255))\n",
        "  return Image.alpha_composite(white_bg.convert(\"RGBA\"), input_image).convert(\"RGB\")\n",
        "\n",
        "# Metrics calculation\n",
        "def calculate_metrics_for_pair(predicted_image_rgba, ground_truth_image_rgba):\n",
        "  if predicted_image_rgba is None or ground_truth_image_rgba is None:\n",
        "    nan = float(\"nan\")\n",
        "    return {\n",
        "      \"PSNR\": nan,\n",
        "      \"SSIM\": nan,\n",
        "      \"LPIPS\": nan,\n",
        "      \"MSE\": nan,\n",
        "      \"RMSE\": nan,\n",
        "      \"NRMSE\": nan,\n",
        "      \"MAE\": nan\n",
        "    }\n",
        "\n",
        "  # Images preprocessing\n",
        "  predicted_image_rgb = convert_to_rgb_with_whitened_transparency(predicted_image_rgba)\n",
        "  ground_truth_image_rgb = convert_to_rgb_with_whitened_transparency(ground_truth_image_rgba)\n",
        "  pred_rgb = np.clip(predicted_image_rgb, 0, 255).astype(np.uint8)\n",
        "  gt_rgb = np.clip(ground_truth_image_rgb, 0, 255).astype(np.uint8)\n",
        "\n",
        "  # Computing basic metrics\n",
        "  psnr_val = psnr_metric(gt_rgb, pred_rgb, data_range=255)\n",
        "  mse_val = mse_metric(gt_rgb, pred_rgb)\n",
        "  rmse_val = nrmse_metric(gt_rgb, pred_rgb, normalization=\"euclidean\") * 255\n",
        "  nrmse_val = nrmse_metric(gt_rgb, pred_rgb)\n",
        "  mae_val = np.mean(np.abs(gt_rgb.astype(np.float32) - pred_rgb.astype(np.float32)))\n",
        "\n",
        "  # SSIM\n",
        "  min_dim = min(gt_rgb.shape[:2])\n",
        "  win_size = min(min_dim, 7)\n",
        "  if win_size % 2 == 0:\n",
        "    win_size -= 1\n",
        "  try:\n",
        "    ssim_val = ssim_metric(\n",
        "      gt_rgb, pred_rgb,\n",
        "      data_range=255,\n",
        "      channel_axis=2,\n",
        "      win_size=win_size,\n",
        "      gaussian_weights=True\n",
        "    )\n",
        "    if ssim_val == 1.0:\n",
        "      ssim_val = float(\"nan\")\n",
        "  except Exception:\n",
        "    ssim_val = float(\"nan\")\n",
        "\n",
        "  # LPIPS\n",
        "  lpips_val = float(\"nan\")\n",
        "  if LPIPS_AVAILABLE:\n",
        "    def to_tensor(img):\n",
        "      img_t = torch.from_numpy(img.astype(np.float32) / 255.0)\n",
        "      img_t = img_t.permute(2, 0, 1).unsqueeze(0) * 2 - 1\n",
        "      return img_t\n",
        "    try:\n",
        "      lpips_val = lpips_alex(to_tensor(pred_rgb), to_tensor(gt_rgb)).item()\n",
        "    except Exception:\n",
        "      lpips_val = float(\"nan\")\n",
        "\n",
        "  return {\n",
        "    \"PSNR\": psnr_val,\n",
        "    \"SSIM\": ssim_val,\n",
        "    \"LPIPS\": lpips_val,\n",
        "    \"MSE\": mse_val,\n",
        "    \"RMSE\": rmse_val,\n",
        "    \"NRMSE\": nrmse_val,\n",
        "    \"MAE\": mae_val\n",
        "  }\n",
        "\n",
        "# def add_pixel_noise_test(image_rgba, num_pixels=10, noise_strength=10):\n",
        "#   noisy_image = image_rgba.copy()\n",
        "#   height, width, _ = noisy_image.shape\n",
        "#   for _ in range(num_pixels):\n",
        "#     y = np.random.randint(0, height)\n",
        "#     x = np.random.randint(0, width)\n",
        "#     noise = np.random.randint(-noise_strength, noise_strength + 1, size=4)  # RGBA\n",
        "#     noisy_image[y, x] = np.clip(noisy_image[y, x] + noise, 0, 255)\n",
        "#   return noisy_image.astype(np.uint8)\n",
        "\n",
        "# Main evaluation loop\n",
        "def run_evaluation():\n",
        "  global MODELS\n",
        "\n",
        "  image_files = get_image_files(images_folder)\n",
        "  if not image_files:\n",
        "    print(f\"No PNG images found in '{images_folder}'. Ensure ground truth images exist in '{images_folder_rotated}'.\")\n",
        "    return pd.DataFrame()\n",
        "\n",
        "  if not MODELS:\n",
        "    print(\"No models found in the MODELS variable.\")\n",
        "    return pd.DataFrame()\n",
        "\n",
        "  all_results = {}  # Average and Median results for each model for these metrics:\n",
        "  metric_names = [\"PSNR\", \"SSIM\", \"LPIPS\", \"MSE\", \"RMSE\", \"NRMSE\", \"MAE\"]\n",
        "\n",
        "  for model_name, predict_function in MODELS.items():\n",
        "    print(f\"\\n--- Evaluating Model: {model_name} ---\")\n",
        "\n",
        "    metric_sums = {m: 0.0 for m in metric_names}\n",
        "    valid_counts = {m: 0 for m in metric_names}\n",
        "    metric_values = {m: [] for m in metric_names}\n",
        "    total_images = 0\n",
        "\n",
        "    for image_name in image_files:\n",
        "      input_image_path = os.path.join(images_folder, image_name)\n",
        "      ground_truth_image_path = os.path.join(images_folder_rotated, image_name)\n",
        "\n",
        "      ground_truth_rgba = load_image_rgba(ground_truth_image_path)\n",
        "      if ground_truth_rgba is None:\n",
        "        raise Exception(f\"Failed to load ground truth {image_name} for model {model_name}.\")\n",
        "\n",
        "      print(f\"Processing {image_name}...\")\n",
        "      total_images += 1\n",
        "\n",
        "      predicted_rgba = predict_function(input_image_path)\n",
        "\n",
        "      if not isinstance(predicted_rgba, np.ndarray):\n",
        "        raise Exception(f\"{model_name} returned non-array for {image_name}\")\n",
        "      else:\n",
        "        if predicted_rgba.ndim == 2:\n",
        "          predicted_rgba = cv2.cvtColor(predicted_rgba, cv2.COLOR_GRAY2RGBA)\n",
        "        elif predicted_rgba.shape[2] == 3:\n",
        "          alpha_channel = np.full((*predicted_rgba.shape[:2], 1), 255, dtype=np.uint8)\n",
        "          predicted_rgba = np.concatenate((predicted_rgba, alpha_channel), axis=2)\n",
        "\n",
        "        if predicted_rgba.shape[2] != 4:\n",
        "          raise Exception(f\"{model_name} output shape not RGBA: {predicted_rgba.shape}\")\n",
        "        else:\n",
        "          predicted_rgba = np.clip(predicted_rgba, 0, 255).astype(np.uint8)\n",
        "\n",
        "          if predicted_rgba.shape[:2] != ground_truth_rgba.shape[:2]:\n",
        "            print(f\"Resizing prediction from {predicted_rgba.shape[:2]} to {ground_truth_rgba.shape[:2]}.\")\n",
        "            predicted_rgba = cv2.resize(predicted_rgba, (ground_truth_rgba.shape[1], ground_truth_rgba.shape[0]), interpolation=cv2.INTER_AREA)\n",
        "\n",
        "          metrics = calculate_metrics_for_pair(predicted_rgba, ground_truth_rgba)\n",
        "\n",
        "      metric_str = \", \".join(f\"{m}={metrics[m]:.4f}\" if np.isfinite(metrics[m]) else f\"{m}=NaN\" for m in metric_names)\n",
        "      print(f\"{image_name} Metrics: {metric_str}\")\n",
        "\n",
        "      if all(np.isfinite(metrics.get(m, float(\"nan\"))) for m in metric_names):\n",
        "        for m in metric_names:\n",
        "          val = metrics[m]\n",
        "          metric_sums[m] += val\n",
        "          metric_values[m].append(val)\n",
        "          valid_counts[m] += 1\n",
        "      else:\n",
        "        print(f\"Skipping {image_name} due to found invalid metric.\")\n",
        "\n",
        "    avg_metrics = {\n",
        "      m: (metric_sums[m] / valid_counts[m]) if valid_counts[m] > 0 else float(\"nan\")\n",
        "      for m in metric_names\n",
        "    }\n",
        "\n",
        "    median_metrics = {\n",
        "      m: float(np.median(metric_values[m])) if metric_values[m] else float(\"nan\")\n",
        "      for m in metric_names\n",
        "    }\n",
        "\n",
        "    all_results[model_name] = {f\"{m}_avg\": avg_metrics[m] for m in metric_names}\n",
        "    all_results[model_name].update({f\"{m}_median\": median_metrics[m] for m in metric_names})\n",
        "\n",
        "    print(f\"Average and Median for {model_name}:\")\n",
        "    for m in metric_names:\n",
        "      print(f\"  {m}: avg={avg_metrics[m]:.4f}, median={median_metrics[m]:.4f} (valid {valid_counts[m]}/{total_images})\")\n",
        "\n",
        "  results_df = pd.DataFrame.from_dict(all_results, orient=\"index\")\n",
        "  print(\"\\n--- Overall Evaluation Results ---\")\n",
        "  print(results_df)\n",
        "  return results_df\n",
        "\n",
        "### --- Models\" prediction functions\" definitions\n",
        "\n",
        "### Model\"s prediction function template for adding new models:\n",
        "# def new_nvs_model_predict(input_img_path: Path to the input image):\n",
        "#   Processes an input image and returns a novel view synthesis prediction.\n",
        "#\n",
        "#   Returns: np.ndarray\n",
        "#     The predicted image as a NumPy array (H, W, 4) in RGBA format, dtype=np.uint8.\n",
        "\n",
        "# --- Passthrough for comparison with models (returns input)\n",
        "def passthrough_predict(input_img_path):\n",
        "  input_image_np_rgba = load_image_rgba(input_img_path)\n",
        "  if input_image_np_rgba is None:\n",
        "    return None\n",
        "  return input_image_np_rgba.copy().astype(np.uint8)\n",
        "\n",
        "# --- Zero123-105000 model definition\n",
        "zero123_105000_pipeline = None\n",
        "\n",
        "def load_zero123_105000_model():\n",
        "  global zero123_105000_pipeline\n",
        "\n",
        "  if zero123_105000_pipeline is None:\n",
        "    model_id = \"kxic/zero123-105000\"\n",
        "    model_path = \"./model_cache/\"\n",
        "    os.makedirs(model_path, exist_ok=True)\n",
        "    local_model_dir = os.path.join(model_path, \"zero123_105000\")\n",
        "\n",
        "    snapshot_download(\n",
        "      model_id,\n",
        "      local_dir=local_model_dir\n",
        "    )\n",
        "\n",
        "    pipe = Zero1to3StableDiffusionPipeline.from_pretrained(\n",
        "      local_model_dir,\n",
        "      torch_dtype=torch.float16,\n",
        "      use_safetensors=False\n",
        "    )\n",
        "\n",
        "    pipe.enable_vae_tiling()\n",
        "    pipe.enable_attention_slicing()\n",
        "    pipe = pipe.to(device)\n",
        "    zero123_105000_pipeline = pipe\n",
        "\n",
        "def zero123_105000_predict(input_img_path):\n",
        "  global zero123_105000_pipeline\n",
        "\n",
        "  load_zero123_105000_model()\n",
        "\n",
        "  if zero123_105000_pipeline is None:\n",
        "    raise Exception(\"'zero123_105000_pipeline' is None\")\n",
        "\n",
        "  input_image = Image.open(input_img_path)\n",
        "  input_image = convert_to_rgb_with_whitened_transparency(input_image)\n",
        "\n",
        "  input_model_size = (256, 256)\n",
        "  input_image_resized = input_image.resize(input_model_size, Image.LANCZOS)\n",
        "\n",
        "  H, W = input_model_size\n",
        "\n",
        "  target_pose = [float(-STATIC_DEGREES), 0.0, 0.0]\n",
        "\n",
        "  print(f\"Zero123 (105000) predicting for {input_img_path} with pose: {target_pose}\")\n",
        "\n",
        "  result_images = zero123_105000_pipeline(\n",
        "    input_imgs=[input_image_resized],\n",
        "    prompt_imgs=[input_image_resized],\n",
        "    poses=[target_pose],\n",
        "    height=H,\n",
        "    width=W,\n",
        "    guidance_scale=3.0,\n",
        "    num_images_per_prompt=1,\n",
        "    num_inference_steps=40\n",
        "  ).images\n",
        "  predicted_image = result_images[0]\n",
        "\n",
        "  predicted_image_rgb = predicted_image.convert(\"RGB\")\n",
        "  predicted_np_rgb = np.array(predicted_image_rgb).astype(np.uint8)\n",
        "\n",
        "  resultDir = \"/content/results/zero123_105000\"\n",
        "  os.makedirs(resultDir, exist_ok=True)\n",
        "  predicted_image_rgb.save(f\"{resultDir}/{os.path.basename(input_img_path)}\")\n",
        "  return predicted_np_rgb\n",
        "### --- End of Zero123-165000 model definition\n",
        "\n",
        "# --- Zero123-165000 model definition\n",
        "zero123_165000_pipeline = None\n",
        "\n",
        "def load_zero123_165000_model():\n",
        "  global zero123_165000_pipeline\n",
        "\n",
        "  if zero123_165000_pipeline is None:\n",
        "    model_id = \"kxic/zero123-165000\"\n",
        "    model_path = \"./model_cache/\"\n",
        "    os.makedirs(model_path, exist_ok=True)\n",
        "    local_model_dir = os.path.join(model_path, \"zero123_165000\")\n",
        "\n",
        "    snapshot_download(\n",
        "      model_id,\n",
        "      local_dir=local_model_dir\n",
        "    )\n",
        "\n",
        "    pipe = Zero1to3StableDiffusionPipeline.from_pretrained(\n",
        "      local_model_dir,\n",
        "      torch_dtype=torch.float16,\n",
        "      use_safetensors=False\n",
        "    )\n",
        "\n",
        "    pipe.enable_vae_tiling()\n",
        "    pipe.enable_attention_slicing()\n",
        "    pipe = pipe.to(device)\n",
        "    zero123_165000_pipeline = pipe\n",
        "\n",
        "def zero123_165000_predict(input_img_path):\n",
        "  global zero123_165000_pipeline\n",
        "\n",
        "  load_zero123_165000_model()\n",
        "\n",
        "  if zero123_165000_pipeline is None:\n",
        "    raise Exception(\"'zero123_165000_pipeline' is None\")\n",
        "\n",
        "  input_image = Image.open(input_img_path)\n",
        "  input_image = convert_to_rgb_with_whitened_transparency(input_image)\n",
        "\n",
        "  input_model_size = (256, 256)\n",
        "  input_image_resized = input_image.resize(input_model_size, Image.LANCZOS)\n",
        "\n",
        "  H, W = input_model_size\n",
        "\n",
        "  target_pose = [float(-STATIC_DEGREES), 0.0, 0.0]\n",
        "\n",
        "  print(f\"Zero123 (165000) predicting for {input_img_path} with pose: {target_pose}\")\n",
        "\n",
        "  result_images = zero123_165000_pipeline(\n",
        "    input_imgs=[input_image_resized],\n",
        "    prompt_imgs=[input_image_resized],\n",
        "    poses=[target_pose],\n",
        "    height=H,\n",
        "    width=W,\n",
        "    guidance_scale=3.0,\n",
        "    num_images_per_prompt=1,\n",
        "    num_inference_steps=40\n",
        "  ).images\n",
        "  predicted_image = result_images[0]\n",
        "\n",
        "  predicted_image_rgb = predicted_image.convert(\"RGB\")\n",
        "  predicted_np_rgb = np.array(predicted_image_rgb).astype(np.uint8)\n",
        "\n",
        "  resultDir = \"/content/results/zero123_165000\"\n",
        "  os.makedirs(resultDir, exist_ok=True)\n",
        "  predicted_image_rgb.save(f\"{resultDir}/{os.path.basename(input_img_path)}\")\n",
        "  return predicted_np_rgb\n",
        "### --- End of Zero123-165000 model definition\n",
        "\n",
        "# --- Zero123-XL model definition\n",
        "zero123_xl_pipeline = None\n",
        "\n",
        "def load_zero123_xl_model():\n",
        "  global zero123_xl_pipeline\n",
        "\n",
        "  if zero123_xl_pipeline is None:\n",
        "    model_id = \"kxic/zero123-xl\"\n",
        "    model_path = \"./model_cache/\"\n",
        "    os.makedirs(model_path, exist_ok=True)\n",
        "    local_model_dir = os.path.join(model_path, \"zero123_xl\")\n",
        "\n",
        "    snapshot_download(\n",
        "      model_id,\n",
        "      local_dir=local_model_dir\n",
        "    )\n",
        "\n",
        "    pipe = Zero1to3StableDiffusionPipeline.from_pretrained(\n",
        "      local_model_dir,\n",
        "      torch_dtype=torch.float16,\n",
        "      use_safetensors=False\n",
        "    )\n",
        "\n",
        "    pipe.enable_vae_tiling()\n",
        "    pipe.enable_attention_slicing()\n",
        "    pipe = pipe.to(device)\n",
        "    zero123_xl_pipeline = pipe\n",
        "\n",
        "def zero123_xl_predict(input_img_path):\n",
        "  global zero123_xl_pipeline\n",
        "\n",
        "  load_zero123_xl_model()\n",
        "\n",
        "  if zero123_xl_pipeline is None:\n",
        "    raise Exception(\"'zero123_xl_pipeline' is None\")\n",
        "\n",
        "  input_image = Image.open(input_img_path)\n",
        "  input_image = convert_to_rgb_with_whitened_transparency(input_image)\n",
        "\n",
        "  input_model_size = (256, 256)\n",
        "  input_image_resized = input_image.resize(input_model_size, Image.LANCZOS)\n",
        "\n",
        "  H, W = input_model_size\n",
        "\n",
        "  target_pose = [float(-STATIC_DEGREES), 0.0, 0.0]\n",
        "\n",
        "  print(f\"Zero123-XL predicting for {input_img_path} with pose: {target_pose}\")\n",
        "\n",
        "  result_images = zero123_xl_pipeline(\n",
        "    input_imgs=[input_image_resized],\n",
        "    prompt_imgs=[input_image_resized],\n",
        "    poses=[target_pose],\n",
        "    height=H,\n",
        "    width=W,\n",
        "    guidance_scale=3.0,\n",
        "    num_images_per_prompt=1,\n",
        "    num_inference_steps=40\n",
        "  ).images\n",
        "  predicted_image = result_images[0]\n",
        "\n",
        "  predicted_image_rgb = predicted_image.convert(\"RGB\")\n",
        "  predicted_np_rgb = np.array(predicted_image_rgb).astype(np.uint8)\n",
        "\n",
        "  resultDir = \"/content/results/zero123_xl\"\n",
        "  os.makedirs(resultDir, exist_ok=True)\n",
        "  predicted_image_rgb.save(f\"{resultDir}/{os.path.basename(input_img_path)}\")\n",
        "  return predicted_np_rgb\n",
        "### --- End of Zero123-XL model definition\n",
        "\n",
        "# --- Stable-Zero123 model definition\n",
        "stable_zero123_pipeline = None\n",
        "\n",
        "def load_stable_zero123_model():\n",
        "  global stable_zero123_pipeline\n",
        "\n",
        "  if stable_zero123_pipeline is None:\n",
        "    model_id = \"kxic/stable-zero123\"\n",
        "    model_path = \"./model_cache/\"\n",
        "    os.makedirs(model_path, exist_ok=True)\n",
        "    local_model_dir = os.path.join(model_path, \"stable_zero123\")\n",
        "\n",
        "    snapshot_download(\n",
        "      model_id,\n",
        "      local_dir=local_model_dir\n",
        "    )\n",
        "\n",
        "    pipe = Zero1to3StableDiffusionPipeline.from_pretrained(\n",
        "      local_model_dir,\n",
        "      torch_dtype=torch.float16,\n",
        "      use_safetensors=False\n",
        "    )\n",
        "\n",
        "    pipe.enable_vae_tiling()\n",
        "    pipe.enable_attention_slicing()\n",
        "    pipe = pipe.to(device)\n",
        "    stable_zero123_pipeline = pipe\n",
        "\n",
        "def stable_zero123_predict(input_img_path):\n",
        "  global stable_zero123_pipeline\n",
        "\n",
        "  load_stable_zero123_model()\n",
        "\n",
        "  if stable_zero123_pipeline is None:\n",
        "    raise Exception(\"'stable_zero123_pipeline' is None\")\n",
        "\n",
        "  input_image = Image.open(input_img_path)\n",
        "  input_image = convert_to_rgb_with_whitened_transparency(input_image)\n",
        "\n",
        "  input_model_size = (256, 256)\n",
        "  input_image_resized = input_image.resize(input_model_size, Image.LANCZOS)\n",
        "\n",
        "  H, W = input_model_size\n",
        "\n",
        "  target_pose = [float(-STATIC_DEGREES), 0.0, 0.0]\n",
        "\n",
        "  print(f\"Stable Zero123 predicting for {input_img_path} with pose: {target_pose}\")\n",
        "\n",
        "  result_images = stable_zero123_pipeline(\n",
        "    input_imgs=[input_image_resized],\n",
        "    prompt_imgs=[input_image_resized],\n",
        "    poses=[target_pose],\n",
        "    height=H,\n",
        "    width=W,\n",
        "    guidance_scale=3.0,\n",
        "    num_images_per_prompt=1,\n",
        "    num_inference_steps=40\n",
        "  ).images\n",
        "  predicted_image = result_images[0]\n",
        "\n",
        "  predicted_image_rgb = predicted_image.convert(\"RGB\")\n",
        "  predicted_np_rgb = np.array(predicted_image_rgb).astype(np.uint8)\n",
        "\n",
        "  resultDir = \"/content/results/stable_zero123\"\n",
        "  os.makedirs(resultDir, exist_ok=True)\n",
        "  predicted_image_rgb.save(f\"{resultDir}/{os.path.basename(input_img_path)}\")\n",
        "  return predicted_np_rgb\n",
        "### --- End of Stable-Zero123 model definition\n",
        "\n",
        "# Models register\n",
        "MODELS[\"Passthrough\"] = passthrough_predict\n",
        "MODELS[\"Zero123_105000\"] = zero123_105000_predict\n",
        "MODELS[\"Zero123_165000\"] = zero123_165000_predict\n",
        "MODELS[\"Zero123_XL\"] = zero123_xl_predict\n",
        "MODELS[\"Stable_Zero123\"] = stable_zero123_predict\n",
        "\n",
        "print(f\"Currently testing rotation degrees: {STATIC_DEGREES}\")\n",
        "print(f\"Input images expected in: {images_folder}\")\n",
        "print(f\"Ground truth images expected in: {images_folder_rotated}\")\n",
        "print(\"----------------------------------------\")\n",
        "\n",
        "evaluation_results_df = run_evaluation()\n",
        "\n",
        "if not evaluation_results_df.empty:\n",
        "  # CSV\n",
        "  evaluation_results_df.to_csv(\"nvs_evaluation_results.csv\")\n",
        "  print(\"\\nResults saved to nvs_evaluation_results.csv\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "UJe4ZSIGTASB",
        "P6M82HYFTCvA",
        "E-1qptgTb8nG"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}